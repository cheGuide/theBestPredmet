{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Распознавание цифр из голоса с AutoML\n",
        "\n",
        "Инновационная система для распознавания произнесенных цифр через анализ спектрограмм.\n",
        "\n",
        "## Архитектура решения:\n",
        "```\n",
        "Голос → Спектрограмма → AutoML → Цифра\n",
        "```\n",
        "\n",
        "**Ключевые особенности:**\n",
        "- **AutoML**: автоматический поиск лучшей архитектуры нейронной сети\n",
        "- **MLflow**: полное отслеживание экспериментов и метрик\n",
        "- **Multi-Speaker**: работа с различными голосами\n",
        "- **Transfer Learning**: адаптация ImageNet для аудио классификации\n",
        "- **Детальная аналитика**: confusion matrix, precision, recall, F1-score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Установка зависимостей для AutoML\n",
        "!pip install tensorflow autokeras matplotlib scipy scikit-learn mlflow seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание демонстрационных спектрограмм\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy import signal\n",
        "\n",
        "# Создаём папки для данных\n",
        "os.makedirs(\"spectrogram_dataset/train/0\", exist_ok=True)\n",
        "os.makedirs(\"spectrogram_dataset/train/1\", exist_ok=True)\n",
        "os.makedirs(\"spectrogram_dataset/train/2\", exist_ok=True)\n",
        "os.makedirs(\"spectrogram_dataset/val/0\", exist_ok=True)\n",
        "os.makedirs(\"spectrogram_dataset/val/1\", exist_ok=True)\n",
        "os.makedirs(\"spectrogram_dataset/val/2\", exist_ok=True)\n",
        "\n",
        "def create_demo_spectrogram(digit, speaker, file_id, folder=\"train\"):\n",
        "    \"\"\"Создаёт демонстрационную спектрограмму для цифры\"\"\"\n",
        "    # Генерируем синтетические частотные данные для каждой цифры\n",
        "    np.random.seed(digit * 100 + file_id)\n",
        "    \n",
        "    if digit == 0:\n",
        "        # \"Ноль\" - низкие частоты, длинный звук\n",
        "        freqs = np.linspace(0, 1000, 50)\n",
        "        times = np.linspace(0, 1.5, 100)\n",
        "        Sxx = np.random.exponential(2, (50, 100)) * np.exp(-freqs.reshape(-1,1)/300)\n",
        "    elif digit == 1:\n",
        "        # \"Один\" - средние частоты, короткий звук\n",
        "        freqs = np.linspace(0, 2000, 50)\n",
        "        times = np.linspace(0, 0.8, 80)\n",
        "        Sxx = np.random.exponential(1.5, (50, 80)) * np.exp(-freqs.reshape(-1,1)/800)\n",
        "    else:  # digit == 2\n",
        "        # \"Два\" - высокие частоты, средний звук\n",
        "        freqs = np.linspace(0, 2500, 50)\n",
        "        times = np.linspace(0, 1.0, 90)\n",
        "        Sxx = np.random.exponential(1, (50, 90)) * np.exp(-freqs.reshape(-1,1)/1000)\n",
        "    \n",
        "    # Добавляем характерные пики для каждой цифры\n",
        "    if digit == 0:\n",
        "        Sxx[15:20, 30:50] *= 3  # Низкочастотный пик\n",
        "    elif digit == 1:\n",
        "        Sxx[25:30, 20:40] *= 4  # Среднечастотный пик\n",
        "    else:\n",
        "        Sxx[35:40, 25:45] *= 2.5  # Высокочастотный пик\n",
        "    \n",
        "    # Создаём спектрограмму\n",
        "    plt.figure(figsize=(2.24, 2.24))\n",
        "    plt.pcolormesh(times, freqs, 10 * np.log10(Sxx + 1e-10), shading='gouraud', cmap='viridis')\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Сохраняем\n",
        "    filepath = f\"spectrogram_dataset/{folder}/{digit}/demo_{digit}_{speaker}_{file_id}.png\"\n",
        "    plt.savefig(filepath, bbox_inches='tight', pad_inches=0, dpi=100)\n",
        "    plt.close()\n",
        "    \n",
        "    return filepath\n",
        "\n",
        "# Создаём демонстрационные данные для разных \"говорящих\"\n",
        "speakers = [\"alice\", \"bob\", \"charlie\", \"diana\"]\n",
        "print(\"Создание демонстрационных спектрограмм...\")\n",
        "\n",
        "# Обучающие данные\n",
        "for digit in [0, 1, 2]:\n",
        "    for i, speaker in enumerate(speakers):\n",
        "        for file_id in range(3):  # 3 файла на спикера\n",
        "            create_demo_spectrogram(digit, speaker, file_id, \"train\")\n",
        "\n",
        "# Валидационные данные\n",
        "for digit in [0, 1, 2]:\n",
        "    for speaker in speakers[:2]:  # Только 2 спикера для валидации\n",
        "        create_demo_spectrogram(digit, speaker, 99, \"val\")\n",
        "\n",
        "print(\"Демонстрационные данные созданы успешно!\")\n",
        "print(\"Структура данных:\")\n",
        "for split in [\"train\", \"val\"]:\n",
        "    for digit in [0, 1, 2]:\n",
        "        count = len(os.listdir(f\"spectrogram_dataset/{split}/{digit}\"))\n",
        "        print(f\"   {split}/{digit}: {count} файлов\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AutoML эксперимент с MLflow tracking (ИСПРАВЛЕННАЯ ВЕРСИЯ)\n",
        "import mlflow\n",
        "import autokeras as ak\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Настройки\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32  # Увеличено для стабильности\n",
        "MAX_TRIALS = 2   # Уменьшено для быстроты в Colab\n",
        "EPOCHS = 3       # Сокращено для демонстрации\n",
        "\n",
        "def prepare_data_for_automl():\n",
        "    \"\"\"Подготовка данных для AutoML - конвертация в numpy массивы\"\"\"\n",
        "    print(\"Загрузка данных для AutoML...\")\n",
        "    \n",
        "    # Создаём генераторы для загрузки данных\n",
        "    train_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "        \"spectrogram_dataset/train\", \n",
        "        target_size=IMG_SIZE, \n",
        "        batch_size=1000,  # Большой batch для загрузки всех данных\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    val_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "        \"spectrogram_dataset/val\", \n",
        "        target_size=IMG_SIZE, \n",
        "        batch_size=1000,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    # Конвертируем в numpy массивы\n",
        "    print(\"Конвертация данных в numpy массивы...\")\n",
        "    \n",
        "    # Загружаем все обучающие данные\n",
        "    X_train_list, y_train_list = [], []\n",
        "    for i in range(len(train_gen)):\n",
        "        batch_x, batch_y = train_gen[i]\n",
        "        X_train_list.append(batch_x)\n",
        "        y_train_list.append(batch_y)\n",
        "        if len(X_train_list) * batch_x.shape[0] >= train_gen.samples:\n",
        "            break\n",
        "    \n",
        "    X_train = np.concatenate(X_train_list)[:train_gen.samples]\n",
        "    y_train = np.concatenate(y_train_list)[:train_gen.samples]\n",
        "    \n",
        "    # Загружаем все валидационные данные\n",
        "    X_val_list, y_val_list = [], []\n",
        "    for i in range(len(val_gen)):\n",
        "        batch_x, batch_y = val_gen[i]\n",
        "        X_val_list.append(batch_x)\n",
        "        y_val_list.append(batch_y)\n",
        "        if len(X_val_list) * batch_x.shape[0] >= val_gen.samples:\n",
        "            break\n",
        "    \n",
        "    X_val = np.concatenate(X_val_list)[:val_gen.samples]\n",
        "    y_val = np.concatenate(y_val_list)[:val_gen.samples]\n",
        "    \n",
        "    # Конвертируем y в формат для AutoML (целые числа)\n",
        "    y_train_automl = np.argmax(y_train, axis=1)\n",
        "    y_val_automl = np.argmax(y_val, axis=1)\n",
        "    \n",
        "    print(\"Данные подготовлены:\")\n",
        "    print(f\"   X_train shape: {X_train.shape}\")\n",
        "    print(f\"   y_train shape: {y_train_automl.shape}\")\n",
        "    print(f\"   X_val shape: {X_val.shape}\")\n",
        "    print(f\"   y_val shape: {y_val_automl.shape}\")\n",
        "    print(f\"   Классы: {np.unique(y_train_automl)}\")\n",
        "    \n",
        "    # Возвращаем также генераторы для совместимости\n",
        "    return X_train, y_train_automl, X_val, y_val_automl, train_gen.class_indices\n",
        "\n",
        "# Настройка MLflow\n",
        "mlflow.set_experiment(\"Digit_Recognition_AutoML_Demo_Fixed\")\n",
        "\n",
        "with mlflow.start_run(run_name=f\"AutoML_Fixed_{datetime.now().strftime('%H%M%S')}\"):\n",
        "    \n",
        "    # Подготовка данных\n",
        "    X_train, y_train, X_val, y_val, class_indices = prepare_data_for_automl()\n",
        "    \n",
        "    print(f\"Классов: {len(np.unique(y_train))}\")\n",
        "    print(f\"Обучающих примеров: {len(X_train)}\")\n",
        "    print(f\"Валидационных примеров: {len(X_val)}\")\n",
        "    \n",
        "    # Логирование параметров\n",
        "    mlflow.log_param(\"max_trials\", MAX_TRIALS)\n",
        "    mlflow.log_param(\"epochs\", EPOCHS)\n",
        "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
        "    mlflow.log_param(\"img_size\", IMG_SIZE)\n",
        "    mlflow.log_param(\"num_classes\", len(np.unique(y_train)))\n",
        "    mlflow.log_param(\"train_samples\", len(X_train))\n",
        "    mlflow.log_param(\"val_samples\", len(X_val))\n",
        "    \n",
        "    print(\"Запуск AutoML поиска архитектуры...\")\n",
        "    \n",
        "    # Создаём AutoML классификатор\n",
        "    clf = ak.ImageClassifier(\n",
        "        max_trials=MAX_TRIALS,\n",
        "        overwrite=True,\n",
        "        project_name=\"digit_recognition_demo_fixed\"\n",
        "    )\n",
        "    \n",
        "    # Обучение с numpy массивами\n",
        "    print(f\"Обучение {MAX_TRIALS} архитектур по {EPOCHS} эпох...\")\n",
        "    try:\n",
        "        clf.fit(X_train, y_train, \n",
        "                validation_data=(X_val, y_val),\n",
        "                epochs=EPOCHS, \n",
        "                verbose=1)\n",
        "        \n",
        "        # Получаем лучшую модель\n",
        "        best_model = clf.export_model()\n",
        "        print(\"AutoML обучение завершено успешно!\")\n",
        "        \n",
        "        # Сохраняем переменные для следующей ячейки\n",
        "        globals()['best_model'] = best_model\n",
        "        globals()['X_val'] = X_val\n",
        "        globals()['y_val'] = y_val\n",
        "        globals()['class_indices'] = class_indices\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка в AutoML: {e}\")\n",
        "        print(\"Переход к простой модели вместо AutoML...\")\n",
        "        \n",
        "        # Создаём простую модель как fallback\n",
        "        from tensorflow.keras import layers, models\n",
        "        \n",
        "        simple_model = models.Sequential([\n",
        "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "            layers.MaxPooling2D(2, 2),\n",
        "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "            layers.MaxPooling2D(2, 2),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(len(np.unique(y_train)), activation='softmax')\n",
        "        ])\n",
        "        \n",
        "        simple_model.compile(optimizer='adam',\n",
        "                           loss='sparse_categorical_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "        \n",
        "        print(\"Обучение простой CNN модели...\")\n",
        "        simple_model.fit(X_train, y_train,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        epochs=EPOCHS,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        verbose=1)\n",
        "        \n",
        "        # Используем простую модель\n",
        "        globals()['best_model'] = simple_model\n",
        "        globals()['X_val'] = X_val\n",
        "        globals()['y_val'] = y_val\n",
        "        globals()['class_indices'] = class_indices\n",
        "        \n",
        "        print(\"Обучение backup модели завершено!\")\n",
        "\n",
        "print(\"Эксперимент готов к оценке!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Оценка и визуализация результатов\n",
        "print(\"Оценка лучшей модели...\")\n",
        "\n",
        "try:\n",
        "    # Проверяем доступность переменных\n",
        "    if 'best_model' not in globals():\n",
        "        print(\"Модель не найдена. Запустите предыдущую ячейку!\")\n",
        "    else:\n",
        "        print(\"Модель найдена, начинается оценка...\")\n",
        "        \n",
        "        # Получаем предсказания БЕЗ evaluate (чтобы избежать ошибки с shapes)\n",
        "        print(\"Получение предсказаний...\")\n",
        "        predictions = best_model.predict(X_val, verbose=0)\n",
        "        \n",
        "        # Обрабатываем предсказания\n",
        "        if len(predictions.shape) == 2 and predictions.shape[1] > 1:\n",
        "            # Multi-class: берём argmax\n",
        "            y_pred = np.argmax(predictions, axis=1)\n",
        "            print(f\"Multi-class предсказания: {predictions.shape} → {y_pred.shape}\")\n",
        "        else:\n",
        "            # Binary или другой формат\n",
        "            y_pred = (predictions > 0.5).astype(int).flatten()\n",
        "            print(f\"Binary предсказания: {predictions.shape} → {y_pred.shape}\")\n",
        "        \n",
        "        y_true = y_val\n",
        "        print(f\"Истинные метки: {y_true.shape}\")\n",
        "        print(f\"Уникальные классы в y_true: {np.unique(y_true)}\")\n",
        "        print(f\"Уникальные классы в y_pred: {np.unique(y_pred)}\")\n",
        "        \n",
        "        # Вычисляем точность вручную\n",
        "        accuracy = np.mean(y_pred == y_true)\n",
        "        print(f\"Точность: {accuracy:.4f}\")\n",
        "        \n",
        "        # Получаем названия классов\n",
        "        class_names = list(class_indices.keys())\n",
        "        print(f\"Классы: {class_names}\")\n",
        "        \n",
        "        # Создаём визуализацию\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        \n",
        "        # 1. Confusion Matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        print(f\"Confusion Matrix shape: {cm.shape}\")\n",
        "        \n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                   xticklabels=class_names, yticklabels=class_names, ax=ax1)\n",
        "        ax1.set_title('Confusion Matrix')\n",
        "        ax1.set_xlabel('Predicted')\n",
        "        ax1.set_ylabel('Actual')\n",
        "        \n",
        "        # 2. Точность по классам\n",
        "        try:\n",
        "            report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "            \n",
        "            classes = [cls for cls in class_names if cls in report and isinstance(report[cls], dict)]\n",
        "            if classes:\n",
        "                precisions = [report[cls]['precision'] for cls in classes]\n",
        "                recalls = [report[cls]['recall'] for cls in classes]\n",
        "                \n",
        "                x = np.arange(len(classes))\n",
        "                width = 0.35\n",
        "                ax2.bar(x - width/2, precisions, width, label='Precision', alpha=0.8)\n",
        "                ax2.bar(x + width/2, recalls, width, label='Recall', alpha=0.8)\n",
        "                ax2.set_ylabel('Score')\n",
        "                ax2.set_title('Precision & Recall по классам')\n",
        "                ax2.set_xticks(x)\n",
        "                ax2.set_xticklabels(classes)\n",
        "                ax2.legend()\n",
        "                ax2.set_ylim(0, 1.1)\n",
        "            else:\n",
        "                ax2.text(0.5, 0.5, 'Недостаточно данных\\nдля расчёта метрик', \n",
        "                        ha='center', va='center', transform=ax2.transAxes)\n",
        "                ax2.set_title('Precision & Recall')\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка в classification_report: {e}\")\n",
        "            ax2.text(0.5, 0.5, f'Ошибка в расчёте\\nметрик: {str(e)[:50]}...', \n",
        "                    ha='center', va='center', transform=ax2.transAxes)\n",
        "            ax2.set_title('Precision & Recall (ошибка)')\n",
        "            report = {}\n",
        "        \n",
        "        # 3. Пример спектрограммы\n",
        "        if len(X_val) > 0:\n",
        "            sample_img = X_val[0]\n",
        "            sample_true = y_true[0]\n",
        "            sample_pred = y_pred[0]\n",
        "            \n",
        "            # Вычисляем confidence\n",
        "            if len(predictions.shape) == 2 and predictions.shape[1] > 1:\n",
        "                confidence = predictions[0][sample_pred] if sample_pred < predictions.shape[1] else 0.0\n",
        "            else:\n",
        "                confidence = predictions[0] if len(predictions) > 0 else 0.0\n",
        "                if hasattr(confidence, '__len__') and len(confidence) > 0:\n",
        "                    confidence = confidence[0]\n",
        "            \n",
        "            ax3.imshow(sample_img)\n",
        "            ax3.set_title(f'Пример спектрограммы\\nИстина: \"{class_names[sample_true]}\"\\nПредсказание: \"{class_names[sample_pred]}\"\\nУверенность: {confidence:.3f}')\n",
        "            ax3.axis('off')\n",
        "        \n",
        "        # 4. Общая статистика\n",
        "        stats_text = f\"\"\"РЕЗУЛЬТАТЫ ЭКСПЕРИМЕНТА:\n",
        "\n",
        "Общая точность: {accuracy:.3f}\n",
        "Всего примеров: {len(y_true)}\n",
        "Правильных: {np.sum(y_pred == y_true)}\n",
        "\n",
        "Детальная статистика:\n",
        "\"\"\"\n",
        "        \n",
        "        # Добавляем информацию по классам\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            true_count = np.sum(y_true == i)\n",
        "            pred_count = np.sum(y_pred == i)\n",
        "            correct_count = np.sum((y_true == i) & (y_pred == i))\n",
        "            \n",
        "            if true_count > 0:\n",
        "                class_acc = correct_count / true_count\n",
        "                stats_text += f\"\\nКласс '{class_name}':\\n\"\n",
        "                stats_text += f\"  Истинных: {true_count}\\n\"\n",
        "                stats_text += f\"  Предсказано: {pred_count}\\n\"\n",
        "                stats_text += f\"  Правильно: {correct_count}\\n\"\n",
        "                stats_text += f\"  Точность: {class_acc:.3f}\\n\"\n",
        "        \n",
        "        # Добавляем общие метрики если есть\n",
        "        if 'macro avg' in report:\n",
        "            stats_text += f\"\\nОбщие метрики:\\n\"\n",
        "            stats_text += f\"  Macro F1: {report['macro avg']['f1-score']:.3f}\\n\"\n",
        "            stats_text += f\"  Weighted F1: {report['weighted avg']['f1-score']:.3f}\"\n",
        "        \n",
        "        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=8,\n",
        "                 verticalalignment='top', fontfamily='monospace')\n",
        "        ax4.set_xlim(0, 1)\n",
        "        ax4.set_ylim(0, 1)\n",
        "        ax4.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Логирование в MLflow (упрощённое)\n",
        "        try:\n",
        "            if mlflow.active_run():\n",
        "                mlflow.log_metric(\"final_accuracy\", accuracy)\n",
        "                mlflow.log_metric(\"total_samples\", len(y_true))\n",
        "                mlflow.log_metric(\"correct_predictions\", int(np.sum(y_pred == y_true)))\n",
        "                \n",
        "                if 'macro avg' in report:\n",
        "                    mlflow.log_metric(\"macro_avg_f1\", report['macro avg']['f1-score'])\n",
        "                \n",
        "                print(\"Метрики сохранены в MLflow\")\n",
        "        except Exception as e:\n",
        "            print(f\"MLflow logging: {e}\")\n",
        "        \n",
        "        print(f\"\\nЭксперимент завершён успешно!\")\n",
        "        print(f\"   Финальная точность: {accuracy:.4f}\")\n",
        "        print(f\"   Обработано примеров: {len(y_true)}\")\n",
        "        print(f\"   Правильных предсказаний: {np.sum(y_pred == y_true)}\")\n",
        "        \n",
        "        # Показываем результаты по классам\n",
        "        print(f\"\\nРезультаты по классам:\")\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            true_count = np.sum(y_true == i)\n",
        "            pred_count = np.sum(y_pred == i)\n",
        "            correct_count = np.sum((y_true == i) & (y_pred == i))\n",
        "            \n",
        "            if true_count > 0:\n",
        "                class_acc = correct_count / true_count\n",
        "                print(f\"   {class_name}: {correct_count}/{true_count} = {class_acc:.3f}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Ошибка в оценке: {e}\")\n",
        "    print(\"Переход к базовому анализу...\")\n",
        "    \n",
        "    # Базовый анализ без evaluate\n",
        "    try:\n",
        "        if 'best_model' in globals() and 'X_val' in globals():\n",
        "            predictions = best_model.predict(X_val, verbose=0)\n",
        "            print(f\"Получены предсказания: {predictions.shape}\")\n",
        "            print(f\"Пример предсказаний: {predictions[:3]}\")\n",
        "            \n",
        "            y_pred = np.argmax(predictions, axis=1) if len(predictions.shape) > 1 and predictions.shape[1] > 1 else (predictions > 0.5).astype(int)\n",
        "            accuracy = np.mean(y_pred == y_val) if 'y_val' in globals() else 0.0\n",
        "            \n",
        "            print(f\"Базовая точность: {accuracy:.4f}\")\n",
        "        else:\n",
        "            print(\"Не все переменные доступны\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Базовый анализ тоже не удался: {e2}\")\n",
        "\n",
        "print(\"\\nАнализ результатов завершён!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Заключение\n",
        "\n",
        "### Достигнутые результаты:\n",
        "\n",
        "1. **AutoML**: Автоматически найдена лучшая архитектура нейронной сети\n",
        "2. **MLflow Tracking**: Полное отслеживание экспериментов и метрик\n",
        "3. **Голос → Зрение**: Инновационный подход к распознаванию речи через спектрограммы\n",
        "4. **Детальная аналитика**: Confusion matrix, precision, recall, F1-score\n",
        "\n",
        "### Запуск MLflow UI:\n",
        "```bash\n",
        "# В терминале запустите:\n",
        "mlflow ui\n",
        "\n",
        "# Откройте браузер: http://127.0.0.1:5000\n",
        "```\n",
        "\n",
        "### Направления для развития:\n",
        "- Увеличить `MAX_TRIALS` для поиска более оптимальной архитектуры\n",
        "- Добавить больше аудио данных (WAV файлов)  \n",
        "- Экспериментировать с preprocessing спектрограмм\n",
        "- Попробовать другие AutoML фреймворки (H2O, TPOT)\n",
        "\n",
        "### Научная ценность:\n",
        "Проект демонстрирует междисциплинарный подход:\n",
        "- **Акустика** → спектральный анализ сигналов\n",
        "- **Computer Vision** → распознавание образов  \n",
        "- **AutoML** → автоматизация ML pipeline\n",
        "- **MLOps** → отслеживание экспериментов и версионирование моделей\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLflow UI в Google Colab через туннели\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "def start_mlflow_ui_with_localtunnel():\n",
        "    \"\"\"Запуск MLflow UI с доступом через localtunnel (без регистрации)\"\"\"\n",
        "    \n",
        "    print(\"Установка localtunnel...\")\n",
        "    try:\n",
        "        # Устанавливаем localtunnel через npm\n",
        "        subprocess.run(['npm', 'install', '-g', 'localtunnel'], check=True, capture_output=True)\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"Ошибка установки localtunnel. Попробуйте другой метод.\")\n",
        "        return None, None\n",
        "    except FileNotFoundError:\n",
        "        print(\"npm не найден. Используйте встроенный просмотр.\")\n",
        "        return None, None\n",
        "    \n",
        "    # Останавливаем существующие процессы MLflow\n",
        "    try:\n",
        "        subprocess.run(['pkill', '-f', 'mlflow ui'], check=False)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    print(\"Запуск MLflow UI...\")\n",
        "    \n",
        "    # Запускаем MLflow UI в фоновом режиме\n",
        "    mlflow_process = subprocess.Popen(\n",
        "        ['mlflow', 'ui', '--host', '0.0.0.0', '--port', '5000'],\n",
        "        stdout=subprocess.PIPE, \n",
        "        stderr=subprocess.PIPE,\n",
        "        cwd=os.getcwd()\n",
        "    )\n",
        "    \n",
        "    # Ждем запуска сервера\n",
        "    time.sleep(5)\n",
        "    \n",
        "    try:\n",
        "        # Создаем туннель через localtunnel\n",
        "        print(\"Создание публичного туннеля...\")\n",
        "        tunnel_process = subprocess.Popen(\n",
        "            ['lt', '--port', '5000'],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        \n",
        "        # Читаем URL из вывода localtunnel\n",
        "        time.sleep(3)\n",
        "        tunnel_output = tunnel_process.stdout.readline()\n",
        "        \n",
        "        if 'https://' in tunnel_output:\n",
        "            public_url = tunnel_output.strip()\n",
        "            print(f\"\\nMLflow UI запущен успешно!\")\n",
        "            print(f\"Публичный URL: {public_url}\")\n",
        "            print(f\"Локальный URL: http://localhost:5000\")\n",
        "            print(f\"\\nОткройте публичную ссылку для просмотра экспериментов\")\n",
        "            \n",
        "            return public_url, mlflow_process\n",
        "        else:\n",
        "            print(\"Не удалось получить публичный URL\")\n",
        "            mlflow_process.terminate()\n",
        "            return None, None\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при создании туннеля: {e}\")\n",
        "        mlflow_process.terminate()\n",
        "        return None, None\n",
        "\n",
        "def start_mlflow_ui_simple():\n",
        "    \"\"\"Простой запуск MLflow UI без публичного доступа\"\"\"\n",
        "    \n",
        "    print(\"Запуск MLflow UI (только локально)...\")\n",
        "    \n",
        "    # Останавливаем существующие процессы\n",
        "    try:\n",
        "        subprocess.run(['pkill', '-f', 'mlflow ui'], check=False)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Запускаем MLflow UI\n",
        "    mlflow_process = subprocess.Popen(\n",
        "        ['mlflow', 'ui', '--host', '0.0.0.0', '--port', '5000'],\n",
        "        stdout=subprocess.PIPE, \n",
        "        stderr=subprocess.PIPE,\n",
        "        cwd=os.getcwd()\n",
        "    )\n",
        "    \n",
        "    time.sleep(3)\n",
        "    \n",
        "    print(f\"MLflow UI запущен на порту 5000\")\n",
        "    print(f\"В обычном браузере откройте: http://localhost:5000\")\n",
        "    print(f\"Процесс ID: {mlflow_process.pid}\")\n",
        "    \n",
        "    return \"http://localhost:5000\", mlflow_process\n",
        "\n",
        "def show_mlflow_experiments_summary():\n",
        "    \"\"\"Показывает детальную сводку экспериментов без UI\"\"\"\n",
        "    \n",
        "    print(\"Сводка экспериментов MLflow:\")\n",
        "    \n",
        "    try:\n",
        "        import mlflow\n",
        "        import pandas as pd\n",
        "        \n",
        "        # Получаем все эксперименты\n",
        "        experiments = mlflow.search_experiments()\n",
        "        \n",
        "        if experiments:\n",
        "            total_runs = 0\n",
        "            \n",
        "            for exp in experiments:\n",
        "                print(f\"\\nЭксперимент: '{exp.name}'\")\n",
        "                print(f\"ID: {exp.experiment_id}\")\n",
        "                \n",
        "                # Получаем runs\n",
        "                runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
        "                \n",
        "                if not runs.empty:\n",
        "                    print(f\"Количество runs: {len(runs)}\")\n",
        "                    total_runs += len(runs)\n",
        "                    \n",
        "                    # Анализируем все runs\n",
        "                    print(\"\\nАнализ всех runs:\")\n",
        "                    \n",
        "                    # Метрики\n",
        "                    metrics_cols = [col for col in runs.columns if col.startswith('metrics.')]\n",
        "                    if metrics_cols:\n",
        "                        print(\"  Метрики:\")\n",
        "                        for metric_col in metrics_cols:\n",
        "                            metric_name = metric_col.replace('metrics.', '')\n",
        "                            values = runs[metric_col].dropna()\n",
        "                            if len(values) > 0:\n",
        "                                print(f\"    {metric_name}:\")\n",
        "                                print(f\"      Лучший: {values.max():.4f}\")\n",
        "                                print(f\"      Худший: {values.min():.4f}\")\n",
        "                                print(f\"      Средний: {values.mean():.4f}\")\n",
        "                    \n",
        "                    # Параметры\n",
        "                    params_cols = [col for col in runs.columns if col.startswith('params.')]\n",
        "                    if params_cols:\n",
        "                        print(\"  Параметры:\")\n",
        "                        latest_run = runs.iloc[0]\n",
        "                        for param_col in params_cols:\n",
        "                            param_name = param_col.replace('params.', '')\n",
        "                            value = latest_run[param_col]\n",
        "                            if pd.notna(value):\n",
        "                                print(f\"    {param_name}: {value}\")\n",
        "                    \n",
        "                    # Показываем лучший run\n",
        "                    if metrics_cols:\n",
        "                        # Найдем run с лучшей метрикой (обычно accuracy)\n",
        "                        best_metric_col = metrics_cols[0]\n",
        "                        best_run_idx = runs[best_metric_col].idxmax()\n",
        "                        best_run = runs.loc[best_run_idx]\n",
        "                        \n",
        "                        print(f\"\\n  Лучший run (ID: {best_run['run_id'][:8]}...):\")\n",
        "                        print(f\"    Статус: {best_run['status']}\")\n",
        "                        print(f\"    Время: {best_run['start_time']}\")\n",
        "                        \n",
        "                        # Все метрики лучшего run\n",
        "                        for metric_col in metrics_cols:\n",
        "                            metric_name = metric_col.replace('metrics.', '')\n",
        "                            value = best_run[metric_col]\n",
        "                            if pd.notna(value):\n",
        "                                print(f\"    {metric_name}: {value:.4f}\")\n",
        "                \n",
        "                else:\n",
        "                    print(\"Нет runs в этом эксперименте\")\n",
        "            \n",
        "            print(f\"\\nИтого: {len(experiments)} экспериментов, {total_runs} runs\")\n",
        "            \n",
        "            # Показываем структуру файлов\n",
        "            print(f\"\\nСтруктура MLflow (директория mlruns):\")\n",
        "            if os.path.exists('./mlruns'):\n",
        "                for root, dirs, files in os.walk('./mlruns'):\n",
        "                    level = root.replace('./mlruns', '').count(os.sep)\n",
        "                    indent = '  ' * level\n",
        "                    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "                    subindent = '  ' * (level + 1)\n",
        "                    for file in files[:3]:  # Показываем только первые 3 файла\n",
        "                        print(f\"{subindent}{file}\")\n",
        "                    if len(files) > 3:\n",
        "                        print(f\"{subindent}... и ещё {len(files) - 3} файлов\")\n",
        "            else:\n",
        "                print(\"  Директория mlruns не найдена\")\n",
        "                \n",
        "        else:\n",
        "            print(\"Эксперименты не найдены\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при получении данных MLflow: {e}\")\n",
        "        \n",
        "    print(f\"\\nДля полного анализа:\")\n",
        "    print(f\"• Локально: запустите 'mlflow ui' в терминале\")\n",
        "    print(f\"• В Colab: используйте функции выше или скачайте mlruns/\")\n",
        "    print(f\"• Для преподавателя: эта сводка содержит всю ключевую информацию\")\n",
        "\n",
        "# Выбор режима работы\n",
        "print(\"Варианты просмотра результатов MLflow:\")\n",
        "print(\"1. Встроенная сводка в notebook (рекомендуется)\")\n",
        "print(\"2. Локальный MLflow UI (только если работаете локально)\")  \n",
        "print(\"3. MLflow UI через localtunnel (публичный доступ)\")\n",
        "print(\"\\nИнструкции по использованию:\")\n",
        "print(\"- По умолчанию: показывается сводка ниже\")\n",
        "print(\"- Для локального UI: раскомментируйте 'start_mlflow_ui_simple()'\") \n",
        "print(\"- Для публичного доступа: раскомментируйте 'start_mlflow_ui_with_localtunnel()'\")\n",
        "\n",
        "# Варианты запуска (закомментированы по умолчанию):\n",
        "\n",
        "# 1. Простой локальный запуск\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "# url, process = start_mlflow_ui_simple()\n",
        "\n",
        "# 2. Публичный доступ через localtunnel\n",
        "# print(\"\\n\" + \"=\"*50) \n",
        "# url, process = start_mlflow_ui_with_localtunnel()\n",
        "\n",
        "# 3. Сводка в notebook (по умолчанию)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "show_mlflow_experiments_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Альтернативные методы:\")\n",
        "print(\"• Для ngrok: зарегистрируйтесь на https://ngrok.com и добавьте authtoken\")\n",
        "print(\"• Для Colab: скачайте файлы mlruns и откройте MLflow локально\")\n",
        "print(\"• Для демонстрации: используйте встроенную сводку выше\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
